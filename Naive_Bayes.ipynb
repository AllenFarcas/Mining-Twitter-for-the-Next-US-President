{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Naive_Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK5y7TPvyjv9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Data preprocessing to removes @usernames,urls,symbols and makes all text lowercase\n",
        "def preprocess_text(text):\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
        "    text = re.sub('@[^\\s]+','', text)\n",
        "    text = text.lower().replace(\"ё\", \"е\")\n",
        "    text = re.sub('[^a-zA-Zа-яА-Я]+',' ', text)\n",
        "    text = re.sub(' +',' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "#from google.colab import files   \n",
        "#uploaded = files.upload()\n",
        "#nltk.download('punkt')\n",
        "\n",
        "#Global variable\n",
        "batchsize = 100000 "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seNKaxWvyjv9"
      },
      "source": [
        "For now I have tried using just the textblob labelled dataset as input. I tried out for smaller 5k dataset and then 1M rows. Even for 1M I was able to do without using partial_fit (batching) since we anyway use the counts of words which comes out to be a sparse matrix. Might have to do batching for the whole dataset though. Will have to replace the dataset with the actual voted labels from Li. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Misyylaiyjv9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c125b42d-656c-46b9-de16-9a6d00850f67"
      },
      "source": [
        "#load data from csv/excel/json\n",
        "#data=pd.read_csv('textblob_sample_5k.csv',encoding = 'unicode_escape')\n",
        "#data=pd.read_csv('textblob_sentiment_1M.csv',encoding = \"ISO-8859–1\")\n",
        "data = pd.read_csv('vote_all_no_conflict.csv')\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>text</th>\n",
              "      <th>SentiStrength</th>\n",
              "      <th>Vader</th>\n",
              "      <th>Textblob</th>\n",
              "      <th>Vote</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1278320583718178816</td>\n",
              "      <td>Business Group Complains Trump H-1B Reform Boosting U.S. Graduates. Big tech is whining that they will have to hire American instead of cheap fore...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1278346691171586049</td>\n",
              "      <td>That moron trump vows to veto the Defense Bill if it includes renaming bases. So once again, military salaries and defense preparedness are second...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1278368975689220097</td>\n",
              "      <td>@JoeBiden Debate President Trump. PROVE you don’t have dementia. #DementiaJoeCantDebate #JoeBidenScaredToDebate</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1278368976960184320</td>\n",
              "      <td>@PamelaStovall6 @ChuckGrassley @realDonaldTrump Democrats are always saving @GOP dinosaurs like grassley after they fvck up</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1278368971314597890</td>\n",
              "      <td>@Jorgensen4POTUS @RealSpikeCohen Just found out about you and so far I love your policies and what you have to say. Ofc I still need to read more ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Tweet ID  ...      Vote\n",
              "0  1278320583718178816  ...  Negative\n",
              "1  1278346691171586049  ...  Negative\n",
              "2  1278368975689220097  ...   Neutral\n",
              "3  1278368976960184320  ...   Neutral\n",
              "4  1278368971314597890  ...  Positive\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt3L49O_yjv-"
      },
      "source": [
        "#data[\"processed_text\"] =  data['full_text'].apply(preprocess_text)\n",
        "data[\"processed_text\"] = data['text'].apply(preprocess_text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuOu1WcWyjv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fcef20ab-2646-4848-ae94-125872dd440e"
      },
      "source": [
        "pd.options.display.max_colwidth = 150\n",
        "data.head()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>text</th>\n",
              "      <th>SentiStrength</th>\n",
              "      <th>Vader</th>\n",
              "      <th>Textblob</th>\n",
              "      <th>Vote</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1278320583718178816</td>\n",
              "      <td>Business Group Complains Trump H-1B Reform Boosting U.S. Graduates. Big tech is whining that they will have to hire American instead of cheap fore...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>business group complains trump h b reform boosting u s graduates big tech is whining that they will have to hire american instead of cheap foreign...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1278346691171586049</td>\n",
              "      <td>That moron trump vows to veto the Defense Bill if it includes renaming bases. So once again, military salaries and defense preparedness are second...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>that moron trump vows to veto the defense bill if it includes renaming bases so once again military salaries and defense preparedness are second i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1278368975689220097</td>\n",
              "      <td>@JoeBiden Debate President Trump. PROVE you don’t have dementia. #DementiaJoeCantDebate #JoeBidenScaredToDebate</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>debate president trump prove you don t have dementia dementiajoecantdebate joebidenscaredtodebate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1278368976960184320</td>\n",
              "      <td>@PamelaStovall6 @ChuckGrassley @realDonaldTrump Democrats are always saving @GOP dinosaurs like grassley after they fvck up</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>democrats are always saving dinosaurs like grassley after they fvck up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1278368971314597890</td>\n",
              "      <td>@Jorgensen4POTUS @RealSpikeCohen Just found out about you and so far I love your policies and what you have to say. Ofc I still need to read more ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>just found out about you and so far i love your policies and what you have to say ofc i still need to read more about your stances and plans but i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Tweet ID  ...                                                                                                                                         processed_text\n",
              "0  1278320583718178816  ...  business group complains trump h b reform boosting u s graduates big tech is whining that they will have to hire american instead of cheap foreign...\n",
              "1  1278346691171586049  ...  that moron trump vows to veto the defense bill if it includes renaming bases so once again military salaries and defense preparedness are second i...\n",
              "2  1278368975689220097  ...                                                      debate president trump prove you don t have dementia dementiajoecantdebate joebidenscaredtodebate\n",
              "3  1278368976960184320  ...                                                                                 democrats are always saving dinosaurs like grassley after they fvck up\n",
              "4  1278368971314597890  ...  just found out about you and so far i love your policies and what you have to say ofc i still need to read more about your stances and plans but i...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrFXgX7gyjv-"
      },
      "source": [
        "#Converting the text into tokens and getting the counts of each token based on the ngrams specified. \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "#cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = nltk.word_tokenize)\n",
        "#cv=CountVectorizer(stop_words='english')\n",
        "text_counts = cv.fit_transform(data['processed_text'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NfG8xM4yjv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac71285-aa41-4d9d-cb02-d357d42bdba4"
      },
      "source": [
        "text_counts\n",
        "#cv.vocabulary_\n",
        "# Import LabelEncoder\n",
        "#from sklearn import preprocessing\n",
        "#creating labelEncoder\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label = le.fit_transform(data['Sentiment'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2475775x247849 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 25453762 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5793hDkByjv-"
      },
      "source": [
        "The three Naive bayes models with support for partial fit are MNB,GNB and BNB. So I have tried implementing the three of them.\n",
        "Multinomial Naive Bayes(MNB) is the one thats particularly good with word counts, so we can try to tune that the most and probably get better accuracy.  \n",
        "I have tried three apporaches:  \n",
        "1. Using the text counts from Count Vectorizer as input\n",
        "2. Using the counts from tf-idf vectorizer as input\n",
        "3. converting the counts from part 1 as tf-idf frequency using transformer and feeding to model  \n",
        "I think approach 2 and 3 are supposed to work equivantely but I noticed a slight increase in accuracy for the MNB model with approach 3 while using the smaller dataset. (couldn't try approch 3 for 1M dataset as memory required was too large)  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSxG-j4syjv-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(text_counts,data['Sentiment'], test_size=0.25, random_state=5)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(text_counts,data['Vote'], test_size=0.25, random_state=5)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtQ0pvfaUKwV"
      },
      "source": [
        "def batch_xy(input, output, batchsize):\n",
        "  for i in range(0, input.shape[0], batchsize):\n",
        "    yield input[i:i + batchsize, :], output[i:i + batchsize]\n",
        "\n",
        "def batch_x(input, batchsize):\n",
        "  for i in range(0, input.shape[0], batchsize):\n",
        "    yield input[i:i + batchsize, :]\n",
        "\n",
        "def report(name, target, pred):\n",
        "  label = \"[\" + name + \"] classification report:\"\n",
        "  print(label)\n",
        "  print(classification_report(target, pred)) \n",
        "\n",
        "def roc(name, model, input, target):\n",
        "  label = \"[\" + name + \"] roc curve:\"\n",
        "  print(label)\n",
        "  plt.figure()\n",
        "  roc_plt = plot_roc_curve(model, input, target)\n",
        "  plt.show()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M19VQtQLyjv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749a2187-fbf7-4a01-b353-8bdf6f3d06c2"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  MNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB.predict(x))\n",
        "\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, Y_test)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Initial MNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "#roc(\"Initial MNB with text counts from Count Vectorizer\", MNB, X_test, Y_test)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score_mnb = 81.01%\n",
            "[Initial MNB with text counts from Count Vectorizer] classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.78      0.84      0.81    192007\n",
            "     Neutral       0.84      0.81      0.82    261925\n",
            "    Positive       0.81      0.77      0.79    165012\n",
            "\n",
            "    accuracy                           0.81    618944\n",
            "   macro avg       0.81      0.81      0.81    618944\n",
            "weighted avg       0.81      0.81      0.81    618944\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GnFAENdyjv_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "bab65860-b220-4601-c641-8cbcda9ad770"
      },
      "source": [
        "#searching for best learning rate and training MNB\n",
        "MNB = MultinomialNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(MNB,parameters)\n",
        "search.fit(X_train[0:0 + batchsize, :],Y_train[0:0 + batchsize]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams =search.best_params_\n",
        "bestparams"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8067ab13b961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fit_prior'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Only run through batchsize to determine the best params (whole dataset takes too much resource)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mbestparams\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbestparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    919\u001b[0m             )\n\u001b[1;32m    920\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_values_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key of type tuple not found and not a MultiIndex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;31m# If key is contained, would have returned by now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: key of type tuple not found and not a MultiIndex"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6u-9CJdyjv_"
      },
      "source": [
        "MNB_best = MultinomialNB(alpha=bestparams['alpha'],fit_prior=bestparams['fit_prior'])\n",
        "#MNB_best.fit(X_train,Y_train)\n",
        "#y_pred = MNB_best.predict(X_test)\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  MNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB_best.predict(x))\n",
        "\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, Y_test)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Best Params MNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "#roc(\"Best Params MNB with text counts from Count Vectorizer\", MNB_best, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6gmcdRqyjv_"
      },
      "source": [
        "My system doesn't have enough RAM to run GaussianNB for 1M entries as it requires to convert X_train to a dense array format.So commented it for now. Can may be try running it on some other compute resource. When I had tried running GNB with a dataset of 5k elements it compartively performed worse that MNB and BNB. But in the tfidf vectorizer method GNB had performed a little better than the other two. I think MNB is the popular model for sentiment analysis so we will probably focus more on MNB and try to optimize that the most. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4jxhBHzyjv_"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "GNB = GaussianNB()\n",
        "\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  GNB.partial_fit(x.todense(), y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, GNB.predict(x.todense()))\n",
        "\n",
        "#GNB.fit(X_train.todense(), Y_train)\n",
        "\n",
        "accuracy_score_gnb = metrics.accuracy_score(y_pred, Y_test)\n",
        "print('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')\n",
        "report(\"Initial GNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "#roc(\"Initial GNB with text counts from Count Vectorizer\", GNB, X_test, Y_test)\n",
        "\n",
        "#GNB does not have parameter search? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szzo_pQPyjv_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "2c596b61-57bd-46ce-e9d0-616f038789f0"
      },
      "source": [
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "BNB = BernoulliNB()\n",
        "\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  BNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, BNB.predict(x))\n",
        "\n",
        "#BNB.fit(X_train, Y_train)\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred,Y_test)\n",
        "print('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Initial BNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "#roc(\"Initial BNB with text counts from Count Vectorizer\", BNB, X_test, Y_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BNB accuracy = 74.69%\n",
            "[Initial BNB with text counts from Count Vectorizer] classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.78      0.71      0.74    192007\n",
            "     Neutral       0.72      0.83      0.77    261925\n",
            "    Positive       0.77      0.65      0.70    165012\n",
            "\n",
            "    accuracy                           0.75    618944\n",
            "   macro avg       0.75      0.73      0.74    618944\n",
            "weighted avg       0.75      0.75      0.74    618944\n",
            "\n",
            "[Initial BNB with text counts from Count Vectorizer] roc curve:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-58a5b6d8ef76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BNB accuracy = '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:4.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score_bnb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial BNB with text counts from Count Vectorizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mroc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial BNB with text counts from Count Vectorizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBNB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-b31c2dcac693>\u001b[0m in \u001b[0;36mroc\u001b[0;34m(name, model, input, target)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mroc_plt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_plot/roc_curve.py\u001b[0m in \u001b[0;36mplot_roc_curve\u001b[0;34m(estimator, X, y, sample_weight, drop_intermediate, response_method, name, ax, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: BernoulliNB should be a binary classifier"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-6oZ_8qEpmW"
      },
      "source": [
        "#searching for best learning rate and training BNB\n",
        "BNB = BernoulliNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(BNB,parameters)\n",
        "search.fit(X_train[0:0 + batchsize, :],Y_train[0:0 + batchsize]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams =search.best_params_\n",
        "bestparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HstWeaJzE9Sc"
      },
      "source": [
        "BNB_best = BernoulliNB(alpha=bestparams['alpha'],fit_prior=bestparams['fit_prior'])\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  BNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, BNB_best.predict(x))\n",
        "\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred,Y_test)\n",
        "print('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Best Params BNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "#roc(\"Best Params BNB with text counts from Count Vectorizer\", BNB_best, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ5F2_ZZyjv_"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\",norm=\"l2\")\n",
        "text_count_2 = tfidf.fit_transform(data['processed_text'])\n",
        "\n",
        "#splitting the data in test and training\n",
        "#x_train, x_test, y_train, y_test = train_test_split(text_count_2, data['Sentiment'],test_size=0.25,random_state=5)\n",
        "x_train, x_test, y_train, y_test = train_test_split(text_count_2, data['Vote'],test_size=0.25,random_state=5)\n",
        "\n",
        "#Models - reinitialize models since running fit on already fitted model may return something weird (I could be wrong on this, not sure how sklearn fit works initially)\n",
        "MNB = MultinomialNB()\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  MNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB.predict(x))\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, y_test)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Initial MNB with tfidf count\", y_test, y_pred)\n",
        "#roc(\"Initial MNB with tfidf count\", MNB, x_test, y_test)\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  BNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred2 = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred2 = np.append(y_pred2, BNB.predict(x))\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred2, y_test)\n",
        "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Initial BNB with tfidf count\", y_test, y_pred2)\n",
        "#roc(\"Initial BNB with tfidf count\", BNB, x_test, y_test)\n",
        "\n",
        "GNB = GaussianNB()\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  GNB.partial_fit(x.todense(), y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred3 = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred3 = np.append(y_pred3, GNB.predict(x.todense()))\n",
        "accuracy_score_gnb = metrics.accuracy_score(y_pred3, y_test)\n",
        "print('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')\n",
        "report(\"Initial GNB with tfidf count\", y_test, y_pred3)\n",
        "#roc(\"Initial GNB with tfidf count\", GNB, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGCUMfsJFfkt"
      },
      "source": [
        "#searching for best params\n",
        "MNB = MultinomialNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(MNB,parameters)\n",
        "search.fit(x_train[0:0 + batchsize, :],y_train[0:0 + batchsize]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams_mnb =search.best_params_\n",
        "print(bestparams_mnb)\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(BNB,parameters)\n",
        "search.fit(x_train[0:0 + batchsize, :],y_train[0:0 + batchsize]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams_bnb =search.best_params_\n",
        "print(bestparams_bnb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7H44UFVGTmF"
      },
      "source": [
        "MNB_best = MultinomialNB(alpha=bestparams_mnb['alpha'],fit_prior=bestparams_mnb['fit_prior'])\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  MNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB_best.predict(x))\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, y_test)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Best params MNB with tfidf count\", y_test, y_pred)\n",
        "#roc(\"Best params MNB with tfidf count\", MNB_best, x_test, y_test)\n",
        "\n",
        "BNB_best = BernoulliNB(alpha=bestparams_bnb['alpha'],fit_prior=bestparams_bnb['fit_prior'])\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  BNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred2 = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred2 = np.append(y_pred2, BNB_best.predict(x))\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred2, y_test)\n",
        "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Best params BNB with tfidf count\", y_test, y_pred2)\n",
        "#roc(\"Best params BNB with tfidf count\", BNB_best, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzIE_kyeyjv_"
      },
      "source": [
        "\n",
        "text_counts.toarray()\n",
        "# Convert raw frequency counts into TF-IDF (Term Frequency -- Inverse Document Frequency) values\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "fooTfmer = TfidfTransformer()\n",
        "\n",
        "# Again, fit and transform\n",
        "docs_tfidf = fooTfmer.fit_transform(text_counts)\n",
        "\n",
        "#splitting the data in test and training\n",
        "#from sklearn.model_selection() import train_test_split()\n",
        "#x_train, x_test, y_train, y_test = train_test_split(docs_tfidf, data['Sentiment'],test_size=0.25,random_state=5)\n",
        "x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(docs_tfidf, data['Vote'],test_size=0.25,random_state=5)\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  MNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB.predict(x))\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, y_test_tf)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Initial MNB with tfidf frequency\", y_test_tf, y_pred)\n",
        "#roc(\"Initial MNB with tfidf frequency\", MNB, x_test_tf, y_test_tf)\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  BNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred2 = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred2 = np.append(y_pred2, BNB.predict(x))\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred2, y_test_tf)\n",
        "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Initial BNB with tfidf frequency\", y_test_tf, y_pred2)\n",
        "#roc(\"Initial BNB with tfidf frequency\", BNB, x_test_tf, y_test_tf)\n",
        "\n",
        "GNB = GaussianNB()\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  GNB.partial_fit(x.todense(), y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred3 = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred3 = np.append(y_pred3, GNB.predict(x.todense()))\n",
        "accuracy_score_gnb = metrics.accuracy_score(y_pred3, y_test_tf)\n",
        "print('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')\n",
        "report(\"Initial GNB with tfidf frequency\", y_test_tf, y_pred3)\n",
        "#roc(\"Initial GNB with tfidf frequency\", GNB, x_test_tf, y_test_tf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbzGHbfUIFjC"
      },
      "source": [
        "#searching for best params\n",
        "MNB = MultinomialNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(MNB,parameters)\n",
        "search.fit(x_train_tf[0:0 + batchsize, :],y_train_tf[0:0 + batchsize]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams_mnb =search.best_params_\n",
        "print(bestparams_mnb)\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(BNB,parameters)\n",
        "search.fit(x_train_tf[0:0 + batchsize, :],y_train_tf[0:0 + batchsize]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams_bnb =search.best_params_\n",
        "print(bestparams_bnb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNyMch4WIOfR"
      },
      "source": [
        "MNB_best = MultinomialNB(alpha=bestparams_mnb['alpha'],fit_prior=bestparams_mnb['fit_prior'])\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  MNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB_best.predict(x))\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, y_test_tf)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Best params MNB with tfidf count\", y_test_tf, y_pred)\n",
        "#roc(\"Best params MNB with tfidf count\", MNB_best, x_test_tf, y_test_tf)\n",
        "\n",
        "BNB_best = BernoulliNB(alpha=bestparams_bnb['alpha'],fit_prior=bestparams_bnb['fit_prior'])\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  BNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred2 = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred2 = np.append(y_pred2, BNB_best.predict(x))\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred2, y_test_tf)\n",
        "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Best params BNB with tfidf count\", y_test_tf, y_pred2)\n",
        "#roc(\"Best params BNB with tfidf count\", BNB_best, x_test_tf, y_test_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re3Iozwdyjv_"
      },
      "source": [
        "Things to do:  \n",
        "1. Run for the whole voted dataset on a larger compute resource(suggested by TA). Depending on need can make use of the partial fit function to do batching. \n",
        "2. Try to form a pipeline of countvectorizer, tfidf-transformer, MNB and then try to tune the parameters (ngram range,tfidf-norm,MNB alpha etc.) for the whole pipeline using GridsearchCV.\n",
        "3. Try out other optimizations and tuning if exists.  \n",
        "4. Analyze the performance of each of the three models(MNB,GNB,BNB) by using metrics like classification report etc. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzAsAetJyjv_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}