{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Naive_Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK5y7TPvyjv9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "11f791f7-6415-4ad5-b294-b02ddb3b0568"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "\n",
        "#Data preprocessing to removes @usernames,urls,symbols and makes all text lowercase\n",
        "def preprocess_text(text):\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
        "    text = re.sub('@[^\\s]+','', text)\n",
        "    text = text.lower().replace(\"ё\", \"е\")\n",
        "    text = re.sub('[^a-zA-Zа-яА-Я]+',' ', text)\n",
        "    text = re.sub(' +',' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "from google.colab import files   \n",
        "uploaded = files.upload()\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Global variable\n",
        "batchsize = 100000 "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9ff7dd02-b77f-4772-b0fc-8b8c363e487e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9ff7dd02-b77f-4772-b0fc-8b8c363e487e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving vote_all_no_conflict.csv to vote_all_no_conflict.csv\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seNKaxWvyjv9"
      },
      "source": [
        "For now I have tried using just the textblob labelled dataset as input. I tried out for smaller 5k dataset and then 1M rows. Even for 1M I was able to do without using partial_fit (batching) since we anyway use the counts of words which comes out to be a sparse matrix. Might have to do batching for the whole dataset though. Will have to replace the dataset with the actual voted labels from Li. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Misyylaiyjv9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d40a1b81-87eb-4196-8997-67319b4f93bf"
      },
      "source": [
        "#load data from csv/excel/json\n",
        "#data=pd.read_csv('textblob_sample_5k.csv',encoding = 'unicode_escape')\n",
        "#data=pd.read_csv('textblob_sentiment_1M.csv',encoding = \"ISO-8859–1\")\n",
        "data = pd.read_csv('vote_all_no_conflict.csv')\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>text</th>\n",
              "      <th>SentiStrength</th>\n",
              "      <th>Vader</th>\n",
              "      <th>Textblob</th>\n",
              "      <th>Vote</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1278320583718178816</td>\n",
              "      <td>Business Group Complains Trump H-1B Reform Boo...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1278346691171586049</td>\n",
              "      <td>That moron trump vows to veto the Defense Bill...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1278368975689220097</td>\n",
              "      <td>@JoeBiden Debate President Trump. PROVE you do...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1278368976960184320</td>\n",
              "      <td>@PamelaStovall6 @ChuckGrassley @realDonaldTrum...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1278368971314597890</td>\n",
              "      <td>@Jorgensen4POTUS @RealSpikeCohen Just found ou...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Tweet ID  ...      Vote\n",
              "0  1278320583718178816  ...  Negative\n",
              "1  1278346691171586049  ...  Negative\n",
              "2  1278368975689220097  ...   Neutral\n",
              "3  1278368976960184320  ...   Neutral\n",
              "4  1278368971314597890  ...  Positive\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt3L49O_yjv-"
      },
      "source": [
        "#data[\"processed_text\"] =  data['full_text'].apply(preprocess_text)\n",
        "data[\"processed_text\"] = data['text'].apply(preprocess_text)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuOu1WcWyjv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b5575f17-bcb7-4d67-eac6-bc2e081e5e0b"
      },
      "source": [
        "pd.options.display.max_colwidth = 150\n",
        "data.head()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet ID</th>\n",
              "      <th>text</th>\n",
              "      <th>SentiStrength</th>\n",
              "      <th>Vader</th>\n",
              "      <th>Textblob</th>\n",
              "      <th>Vote</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1278320583718178816</td>\n",
              "      <td>Business Group Complains Trump H-1B Reform Boosting U.S. Graduates. Big tech is whining that they will have to hire American instead of cheap fore...</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>business group complains trump h b reform boosting u s graduates big tech is whining that they will have to hire american instead of cheap foreign...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1278346691171586049</td>\n",
              "      <td>That moron trump vows to veto the Defense Bill if it includes renaming bases. So once again, military salaries and defense preparedness are second...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Negative</td>\n",
              "      <td>that moron trump vows to veto the defense bill if it includes renaming bases so once again military salaries and defense preparedness are second i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1278368975689220097</td>\n",
              "      <td>@JoeBiden Debate President Trump. PROVE you don’t have dementia. #DementiaJoeCantDebate #JoeBidenScaredToDebate</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>debate president trump prove you don t have dementia dementiajoecantdebate joebidenscaredtodebate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1278368976960184320</td>\n",
              "      <td>@PamelaStovall6 @ChuckGrassley @realDonaldTrump Democrats are always saving @GOP dinosaurs like grassley after they fvck up</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>democrats are always saving dinosaurs like grassley after they fvck up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1278368971314597890</td>\n",
              "      <td>@Jorgensen4POTUS @RealSpikeCohen Just found out about you and so far I love your policies and what you have to say. Ofc I still need to read more ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>just found out about you and so far i love your policies and what you have to say ofc i still need to read more about your stances and plans but i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Tweet ID  ...                                                                                                                                         processed_text\n",
              "0  1278320583718178816  ...  business group complains trump h b reform boosting u s graduates big tech is whining that they will have to hire american instead of cheap foreign...\n",
              "1  1278346691171586049  ...  that moron trump vows to veto the defense bill if it includes renaming bases so once again military salaries and defense preparedness are second i...\n",
              "2  1278368975689220097  ...                                                      debate president trump prove you don t have dementia dementiajoecantdebate joebidenscaredtodebate\n",
              "3  1278368976960184320  ...                                                                                 democrats are always saving dinosaurs like grassley after they fvck up\n",
              "4  1278368971314597890  ...  just found out about you and so far i love your policies and what you have to say ofc i still need to read more about your stances and plans but i...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrFXgX7gyjv-"
      },
      "source": [
        "#Converting the text into tokens and getting the counts of each token based on the ngrams specified. \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "#cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = nltk.word_tokenize)\n",
        "#cv=CountVectorizer(stop_words='english')\n",
        "text_counts = cv.fit_transform(data['processed_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NfG8xM4yjv-"
      },
      "source": [
        "text_counts\n",
        "#cv.vocabulary_\n",
        "# Import LabelEncoder\n",
        "#from sklearn import preprocessing\n",
        "#creating labelEncoder\n",
        "#le = preprocessing.LabelEncoder()\n",
        "#label = le.fit_transform(data['Sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5793hDkByjv-"
      },
      "source": [
        "The three Naive bayes models with support for partial fit are MNB,GNB and BNB. So I have tried implementing the three of them.\n",
        "Multinomial Naive Bayes(MNB) is the one thats particularly good with word counts, so we can try to tune that the most and probably get better accuracy.  \n",
        "I have tried three apporaches:  \n",
        "1. Using the text counts from Count Vectorizer as input\n",
        "2. Using the counts from tf-idf vectorizer as input\n",
        "3. converting the counts from part 1 as tf-idf frequency using transformer and feeding to model  \n",
        "I think approach 2 and 3 are supposed to work equivantely but I noticed a slight increase in accuracy for the MNB model with approach 3 while using the smaller dataset. (couldn't try approch 3 for 1M dataset as memory required was too large)  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSxG-j4syjv-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(text_counts,data['Sentiment'], test_size=0.25, random_state=5)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(text_counts,data['Vote'], test_size=0.25, random_state=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtQ0pvfaUKwV"
      },
      "source": [
        "def batch_xy(input, output, batchsize):\n",
        "  for i in range(0, input.shape[0], batchsize):\n",
        "    yield input[i:i + batchsize, :], output[i:i + batchsize]\n",
        "\n",
        "def batch_x(input, batchsize):\n",
        "  for i in range(0, input.shape[0], batchsize):\n",
        "    yield input[i:i + batchsize, :]\n",
        "\n",
        "def report(name, target, pred):\n",
        "  label = \"[\" + name + \"] classification report:\"\n",
        "  print(label)\n",
        "  print(classification_report(target, pred)) \n",
        "\n",
        "def roc(name, model, input, target):\n",
        "  label = \"[\" + name + \"] roc curve:\"\n",
        "  print(label)\n",
        "  plt.figure()\n",
        "  roc_plt = plot_roc_curve(model, input, target)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M19VQtQLyjv_"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  MNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB.predict(x))\n",
        "\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, Y_test)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Initial MNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "roc(\"Initial MNB with text counts from Count Vectorizer\", MNB, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GnFAENdyjv_"
      },
      "source": [
        "#searching for best learning rate and training MNB\n",
        "MNB = MultinomialNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(MNB,parameters)\n",
        "search.fit(X_train[0:0 + batchsize, :],Y_train[0:0 + batchsize, :]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams =search.best_params_\n",
        "bestparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6u-9CJdyjv_"
      },
      "source": [
        "MNB_best = MultinomialNB(alpha=bestparams['alpha'],fit_prior=bestparams['fit_prior'])\n",
        "#MNB_best.fit(X_train,Y_train)\n",
        "#y_pred = MNB_best.predict(X_test)\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  MNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB_best.predict(x))\n",
        "\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, Y_test)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Best Params MNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "roc(\"Best Params MNB with text counts from Count Vectorizer\", MNB_best, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6gmcdRqyjv_"
      },
      "source": [
        "My system doesn't have enough RAM to run GaussianNB for 1M entries as it requires to convert X_train to a dense array format.So commented it for now. Can may be try running it on some other compute resource. When I had tried running GNB with a dataset of 5k elements it compartively performed worse that MNB and BNB. But in the tfidf vectorizer method GNB had performed a little better than the other two. I think MNB is the popular model for sentiment analysis so we will probably focus more on MNB and try to optimize that the most. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4jxhBHzyjv_"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "GNB = GaussianNB()\n",
        "\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  GNB.partial_fit(x.todense(), y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, GNB.predict(x.todense()))\n",
        "\n",
        "#GNB.fit(X_train.todense(), Y_train)\n",
        "\n",
        "accuracy_score_gnb = metrics.accuracy_score(y_pred, Y_test)\n",
        "print('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')\n",
        "report(\"Initial GNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "roc(\"Initial GNB with text counts from Count Vectorizer\", GNB, X_test, Y_test)\n",
        "\n",
        "#GNB does not have parameter search? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szzo_pQPyjv_"
      },
      "source": [
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "BNB = BernoulliNB()\n",
        "\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  BNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, BNB.predict(x))\n",
        "\n",
        "#BNB.fit(X_train, Y_train)\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred,Y_test)\n",
        "print('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Initial BNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "roc(\"Initial BNB with text counts from Count Vectorizer\", BNB, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-6oZ_8qEpmW"
      },
      "source": [
        "#searching for best learning rate and training BNB\n",
        "BNB = BernoulliNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(BNB,parameters)\n",
        "search.fit(X_train[0:0 + batchsize, :],Y_train[0:0 + batchsize, :]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams =search.best_params_\n",
        "bestparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HstWeaJzE9Sc"
      },
      "source": [
        "BNB_best = BernoulliNB(alpha=bestparams['alpha'],fit_prior=bestparams['fit_prior'])\n",
        "for x, y in batch_xy(X_train, Y_train, batchsize):\n",
        "  BNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "  \n",
        "y_pred = np.array([])\n",
        "for x in batch_x(X_test, batchsize):\n",
        "  y_pred = np.append(y_pred, BNB_best.predict(x))\n",
        "\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred,Y_test)\n",
        "print('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Best Params BNB with text counts from Count Vectorizer\", Y_test, y_pred)\n",
        "roc(\"Best Params BNB with text counts from Count Vectorizer\", BNB_best, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ5F2_ZZyjv_"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\",norm=\"l2\")\n",
        "text_count_2 = tfidf.fit_transform(data['processed_text'])\n",
        "\n",
        "#splitting the data in test and training\n",
        "#x_train, x_test, y_train, y_test = train_test_split(text_count_2, data['Sentiment'],test_size=0.25,random_state=5)\n",
        "x_train, x_test, y_train, y_test = train_test_split(text_count_2, data['Vote'],test_size=0.25,random_state=5)\n",
        "\n",
        "#Models - reinitialize models since running fit on already fitted model may return something weird (I could be wrong on this, not sure how sklearn fit works initially)\n",
        "MNB = MultinomialNB()\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  MNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB.predict(x))\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, y_test)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Initial MNB with tfidf count\", y_test, y_pred)\n",
        "roc(\"Initial MNB with tfidf count\", MNB, x_test, y_test)\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  BNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred2 = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred2 = np.append(y_pred2, BNB.predict(x))\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred2, y_test)\n",
        "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Initial BNB with tfidf count\", y_test, y_pred2)\n",
        "roc(\"Initial BNB with tfidf count\", BNB, x_test, y_test)\n",
        "\n",
        "GNB = GaussianNB()\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  GNB.partial_fit(x.todense(), y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred3 = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred3 = np.append(y_pred3, GNB.predict(x.todense()))\n",
        "accuracy_score_gnb = metrics.accuracy_score(y_pred3, y_test)\n",
        "print('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')\n",
        "report(\"Initial GNB with tfidf count\", y_test, y_pred3)\n",
        "roc(\"Initial GNB with tfidf count\", GNB, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGCUMfsJFfkt"
      },
      "source": [
        "#searching for best params\n",
        "MNB = MultinomialNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(MNB,parameters)\n",
        "search.fit(x_train[0:0 + batchsize, :],y_train[0:0 + batchsize, :]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams_mnb =search.best_params_\n",
        "print(bestparams_mnb)\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(BNB,parameters)\n",
        "search.fit(x_train[0:0 + batchsize, :],y_train[0:0 + batchsize, :]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams_bnb =search.best_params_\n",
        "print(bestparams_bnb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7H44UFVGTmF"
      },
      "source": [
        "MNB_best = MultinomialNB(alpha=bestparams_mnb['alpha'],fit_prior=bestparams_mnb['fit_prior'])\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  MNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB_best.predict(x))\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, y_test)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Best params MNB with tfidf count\", y_test, y_pred)\n",
        "roc(\"Best params MNB with tfidf count\", MNB_best, x_test, y_test)\n",
        "\n",
        "BNB_best = BernoulliNB(alpha=bestparams_bnb['alpha'],fit_prior=bestparams_bnb['fit_prior'])\n",
        "for x, y in batch_xy(x_train, y_train, batchsize):\n",
        "  BNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred2 = np.array([])\n",
        "for x in batch_x(x_test, batchsize):\n",
        "  y_pred2 = np.append(y_pred2, BNB_best.predict(x))\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred2, y_test)\n",
        "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Best params BNB with tfidf count\", y_test, y_pred2)\n",
        "roc(\"Best params BNB with tfidf count\", BNB_best, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzIE_kyeyjv_"
      },
      "source": [
        "\n",
        "text_counts.toarray()\n",
        "# Convert raw frequency counts into TF-IDF (Term Frequency -- Inverse Document Frequency) values\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "fooTfmer = TfidfTransformer()\n",
        "\n",
        "# Again, fit and transform\n",
        "docs_tfidf = fooTfmer.fit_transform(text_counts)\n",
        "\n",
        "#splitting the data in test and training\n",
        "#from sklearn.model_selection() import train_test_split()\n",
        "#x_train, x_test, y_train, y_test = train_test_split(docs_tfidf, data['Sentiment'],test_size=0.25,random_state=5)\n",
        "x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(docs_tfidf, data['Vote'],test_size=0.25,random_state=5)\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  MNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB.predict(x))\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, y_test_tf)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Initial MNB with tfidf frequency\", y_test_tf, y_pred)\n",
        "roc(\"Initial MNB with tfidf frequency\", MNB, x_test_tf, y_test_tf)\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  BNB.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred2 = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred2 = np.append(y_pred2, BNB.predict(x))\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred2, y_test_tf)\n",
        "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Initial BNB with tfidf frequency\", y_test_tf, y_pred2)\n",
        "roc(\"Initial BNB with tfidf frequency\", BNB, x_test_tf, y_test_tf)\n",
        "\n",
        "GNB = GaussianNB()\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  GNB.partial_fit(x.todense(), y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred3 = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred3 = np.append(y_pred3, GNB.predict(x.todense()))\n",
        "accuracy_score_gnb = metrics.accuracy_score(y_pred3, y_test_tf)\n",
        "print('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')\n",
        "report(\"Initial GNB with tfidf frequency\", y_test_tf, y_pred3)\n",
        "roc(\"Initial GNB with tfidf frequency\", GNB, x_test_tf, y_test_tf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbzGHbfUIFjC"
      },
      "source": [
        "#searching for best params\n",
        "MNB = MultinomialNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(MNB,parameters)\n",
        "search.fit(x_train_tf[0:0 + batchsize, :],y_train_tf[0:0 + batchsize, :]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams_mnb =search.best_params_\n",
        "print(bestparams_mnb)\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
        "search =  GridSearchCV(BNB,parameters)\n",
        "search.fit(x_train_tf[0:0 + batchsize, :],y_train_tf[0:0 + batchsize, :]) #Only run through batchsize to determine the best params (whole dataset takes too much resource)\n",
        "bestparams_bnb =search.best_params_\n",
        "print(bestparams_bnb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNyMch4WIOfR"
      },
      "source": [
        "MNB_best = MultinomialNB(alpha=bestparams_mnb['alpha'],fit_prior=bestparams_mnb['fit_prior'])\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  MNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred = np.append(y_pred, MNB_best.predict(x))\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, y_test_tf)\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "report(\"Best params MNB with tfidf count\", y_test_tf, y_pred)\n",
        "roc(\"Best params MNB with tfidf count\", MNB_best, x_test_tf, y_test_tf)\n",
        "\n",
        "BNB_best = BernoulliNB(alpha=bestparams_bnb['alpha'],fit_prior=bestparams_bnb['fit_prior'])\n",
        "for x, y in batch_xy(x_train_tf, y_train_tf, batchsize):\n",
        "  BNB_best.partial_fit(x, y, classes=['Positive','Neutral','Negative'])\n",
        "y_pred2 = np.array([])\n",
        "for x in batch_x(x_test_tf, batchsize):\n",
        "  y_pred2 = np.append(y_pred2, BNB_best.predict(x))\n",
        "accuracy_score_bnb = metrics.accuracy_score(y_pred2, y_test_tf)\n",
        "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
        "report(\"Best params BNB with tfidf count\", y_test_tf, y_pred2)\n",
        "roc(\"Best params BNB with tfidf count\", BNB_best, x_test_tf, y_test_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re3Iozwdyjv_"
      },
      "source": [
        "Things to do:  \n",
        "1. Run for the whole voted dataset on a larger compute resource(suggested by TA). Depending on need can make use of the partial fit function to do batching. \n",
        "2. Try to form a pipeline of countvectorizer, tfidf-transformer, MNB and then try to tune the parameters (ngram range,tfidf-norm,MNB alpha etc.) for the whole pipeline using GridsearchCV.\n",
        "3. Try out other optimizations and tuning if exists.  \n",
        "4. Analyze the performance of each of the three models(MNB,GNB,BNB) by using metrics like classification report etc. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzAsAetJyjv_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}