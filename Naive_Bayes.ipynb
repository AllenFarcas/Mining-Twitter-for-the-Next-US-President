{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Data preprocessing to removes @usernames,urls,symbols and makes all text lowercase\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я]+',' ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now I have tried using just the textblob labelled dataset as input. I tried out for smaller 5k dataset and then 1M rows. Even for 1M I was able to do without using partial_fit (batching) since we anyway use the counts of words which comes out to be a sparse matrix. Might have to do batching for the whole dataset though. Will have to replace the dataset with the actual voted labels from Li. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_str</th>\n",
       "      <th>full_text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1278320583718178816</td>\n",
       "      <td>Business Group Complains Trump H-1B Reform Boo...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1278310669885026305</td>\n",
       "      <td>\"Who's the absent candidate now?...'Sleepy Joe...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1278346691171586049</td>\n",
       "      <td>That moron trump vows to veto the Defense Bill...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1278368973948694528</td>\n",
       "      <td>@ClareTyne @mesainy @HKrassenstein @realDonald...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1278303504071905282</td>\n",
       "      <td>1. Funny how Biden barely criticizes Putin, an...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_str                                          full_text  \\\n",
       "0  1278320583718178816  Business Group Complains Trump H-1B Reform Boo...   \n",
       "1  1278310669885026305  \"Who's the absent candidate now?...'Sleepy Joe...   \n",
       "2  1278346691171586049  That moron trump vows to veto the Defense Bill...   \n",
       "3  1278368973948694528  @ClareTyne @mesainy @HKrassenstein @realDonald...   \n",
       "4  1278303504071905282  1. Funny how Biden barely criticizes Putin, an...   \n",
       "\n",
       "  Sentiment  \n",
       "0  Negative  \n",
       "1  Positive  \n",
       "2  Negative  \n",
       "3   Neutral  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data from csv/excel/json\n",
    "#data=pd.read_csv('textblob_sample_5k.csv',encoding = 'unicode_escape')\n",
    "data=pd.read_csv('textblob_sentiment_1M.csv',encoding = \"ISO-8859–1\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"processed_text\"] =  data['full_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_str</th>\n",
       "      <th>full_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1278320583718178816</td>\n",
       "      <td>Business Group Complains Trump H-1B Reform Boosting U.S. Graduates. Big tech is whining that they will have to hire American instead of cheap fore...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>business group complains trump h b reform boosting u s graduates big tech is whining that they will have to hire american instead of cheap foreign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1278310669885026305</td>\n",
       "      <td>\"Who's the absent candidate now?...'Sleepy Joe' is signaling that he's very much awake -- and dialed into a moment where Trump's leadership is rip...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>who s the absent candidate now sleepy joe is signaling that he s very much awake and dialed into a moment where trump s leadership is ripe for que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1278346691171586049</td>\n",
       "      <td>That moron trump vows to veto the Defense Bill if it includes renaming bases. So once again, military salaries and defense preparedness are second...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>that moron trump vows to veto the defense bill if it includes renaming bases so once again military salaries and defense preparedness are second i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1278368973948694528</td>\n",
       "      <td>@ClareTyne @mesainy @HKrassenstein @realDonaldTrump @NYCMayor @JoeBiden Yep.  Torturing that guy because the Dems donât care.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>yep torturing that guy because the dems don t care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1278303504071905282</td>\n",
       "      <td>1. Funny how Biden barely criticizes Putin, and on the China virus doesnât criticize Xi.Â  Instead, he smears Trump.Â  The reason is the Democra...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>funny how biden barely criticizes putin and on the china virus doesn t criticize xi instead he smears trump the reason is the democrats don t real...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id_str  \\\n",
       "0  1278320583718178816   \n",
       "1  1278310669885026305   \n",
       "2  1278346691171586049   \n",
       "3  1278368973948694528   \n",
       "4  1278303504071905282   \n",
       "\n",
       "                                                                                                                                               full_text  \\\n",
       "0  Business Group Complains Trump H-1B Reform Boosting U.S. Graduates. Big tech is whining that they will have to hire American instead of cheap fore...   \n",
       "1  \"Who's the absent candidate now?...'Sleepy Joe' is signaling that he's very much awake -- and dialed into a moment where Trump's leadership is rip...   \n",
       "2  That moron trump vows to veto the Defense Bill if it includes renaming bases. So once again, military salaries and defense preparedness are second...   \n",
       "3                        @ClareTyne @mesainy @HKrassenstein @realDonaldTrump @NYCMayor @JoeBiden Yep.  Torturing that guy because the Dems donât care.   \n",
       "4  1. Funny how Biden barely criticizes Putin, and on the China virus doesnât criticize Xi.Â  Instead, he smears Trump.Â  The reason is the Democra...   \n",
       "\n",
       "  Sentiment  \\\n",
       "0  Negative   \n",
       "1  Positive   \n",
       "2  Negative   \n",
       "3   Neutral   \n",
       "4  Positive   \n",
       "\n",
       "                                                                                                                                          processed_text  \n",
       "0  business group complains trump h b reform boosting u s graduates big tech is whining that they will have to hire american instead of cheap foreign...  \n",
       "1  who s the absent candidate now sleepy joe is signaling that he s very much awake and dialed into a moment where trump s leadership is ripe for que...  \n",
       "2  that moron trump vows to veto the defense bill if it includes renaming bases so once again military salaries and defense preparedness are second i...  \n",
       "3                                                                                                     yep torturing that guy because the dems don t care  \n",
       "4  funny how biden barely criticizes putin and on the china virus doesn t criticize xi instead he smears trump the reason is the democrats don t real...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 150\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the text into tokens and getting the counts of each token based on the ngrams specified. \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "#cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = nltk.word_tokenize)\n",
    "#cv=CountVectorizer(stop_words='english')\n",
    "text_counts = cv.fit_transform(data['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1048575x158002 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12277549 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts\n",
    "#cv.vocabulary_\n",
    "# Import LabelEncoder\n",
    "#from sklearn import preprocessing\n",
    "#creating labelEncoder\n",
    "#le = preprocessing.LabelEncoder()\n",
    "#label = le.fit_transform(data['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three Naive bayes models with support for partial fit are MNB,GNB and BNB. So I have tried implementing the three of them.\n",
    "Multinomial Naive Bayes(MNB) is the one thats particularly good with word counts, so we can try to tune that the most and probably get better accuracy.  \n",
    "I have tried three apporaches:  \n",
    "1. Using the text counts from Count Vectorizer as input\n",
    "2. Using the counts from tf-idf vectorizer as input\n",
    "3. converting the counts from part 1 as tf-idf frequency using transformer and feeding to model  \n",
    "I think approach 2 and 3 are supposed to work equivantely but I noticed a slight increase in accuracy for the MNB model with approach 3 while using the smaller dataset. (couldn't try approch 3 for 1M dataset as memory required was too large)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts,data['Sentiment'], test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7616004943847656"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "from sklearn import metrics\n",
    "y_pred = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(y_pred, Y_test)\n",
    "accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1, 'fit_prior': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#searching for best learning rate and training MNB\n",
    "MNB = MultinomialNB()\n",
    "parameters = {'alpha':[1,0.5,0.3,0.1,0.01],'fit_prior':(True,False)}\n",
    "search =  GridSearchCV(MNB,parameters)\n",
    "search.fit(X_train,Y_train)\n",
    "bestparams =search.best_params_\n",
    "bestparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676010131835938"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB_best = MultinomialNB(alpha=bestparams['alpha'],fit_prior=bestparams['fit_prior'])\n",
    "MNB_best.fit(X_train,Y_train)\n",
    "y_pred = MNB_best.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(y_pred, Y_test)\n",
    "accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My system doesn't have enough RAM to run GaussianNB for 1M entries as it requires to convert X_train to a dense array format.So commented it for now. Can may be try running it on some other compute resource. When I had tried running GNB with a dataset of 5k elements it compartively performed worse that MNB and BNB. But in the tfidf vectorizer method GNB had performed a little better than the other two. I think MNB is the popular model for sentiment analysis so we will probably focus more on MNB and try to optimize that the most. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.naive_bayes import GaussianNB\\nGNB = GaussianNB()\\nGNB.fit(X_train.todense(), Y_train)\\naccuracy_score = metrics.accuracy_score(GNB.predict(X_test.todense()),Y_test)\\naccuracy_score\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNB = GaussianNB()\n",
    "GNB.fit(X_train.todense(), Y_train)\n",
    "accuracy_score = metrics.accuracy_score(GNB.predict(X_test.todense()),Y_test)\n",
    "accuracy_score\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB accuracy = 75.05%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "accuracy_score_bnb = metrics.accuracy_score(BNB.predict(X_test),Y_test)\n",
    "print('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score_mnb = 69.38%\n",
      "accuracy_score_bnb = 75.51%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\",norm=\"l2\")\n",
    "text_count_2 = tfidf.fit_transform(data['processed_text'])\n",
    "\n",
    "#splitting the data in test and training\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_2, data['Sentiment'],test_size=0.25,random_state=5)\n",
    "\n",
    "#Models\n",
    "MNB.fit(x_train, y_train)\n",
    "accuracy_score_mnb = metrics.accuracy_score(MNB.predict(x_test), y_test)\n",
    "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
    "\n",
    "BNB.fit(x_train, y_train)\n",
    "accuracy_score_bnb = metrics.accuracy_score(BNB.predict(x_test), y_test)\n",
    "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
    "\n",
    "#GNB.fit(x_train.todense(), y_train)\n",
    "#accuracy_score_gnb = metrics.accuracy_score(GNB.predict(x_test.todense()), y_test)\n",
    "#print('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntext_counts.toarray()\\n# Convert raw frequency counts into TF-IDF (Term Frequency -- Inverse Document Frequency) values\\nfrom sklearn.feature_extraction.text import TfidfTransformer\\nfooTfmer = TfidfTransformer()\\n\\n# Again, fit and transform\\ndocs_tfidf = fooTfmer.fit_transform(text_counts)\\n\\n#splitting the data in test and training\\n#from sklearn.model_selection() import train_test_split()\\nx_train, x_test, y_train, y_test = train_test_split(docs_tfidf, data['Sentiment'],test_size=0.25,random_state=5)\\n\\n#defining the model\\n#compilimg the model -> we are going to use already used models GNB, MNB, CNB, BNB\\n#fitting the model\\nMNB.fit(x_train, y_train)\\naccuracy_score_mnb = metrics.accuracy_score(MNB.predict(x_test), y_test)\\nprint('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\\n\\nBNB.fit(x_train, y_train)\\naccuracy_score_bnb = metrics.accuracy_score(BNB.predict(x_test), y_test)\\nprint('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\\n\\n#CNB.fit(x_train, y_train)\\n#accuracy_score_cnb = metrics.accuracy_score(CNB.predict(x_test), y_test)\\n#print('accuracy_score_cnb = '+str('{:4.2f}'.format(accuracy_score_cnb*100))+'%')\\n\\n#GNB.fit(x_train.todense(), y_train)\\n#accuracy_score_gnb = metrics.accuracy_score(GNB.predict(x_test.todense()), y_test)\\n#print('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "text_counts.toarray()\n",
    "# Convert raw frequency counts into TF-IDF (Term Frequency -- Inverse Document Frequency) values\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "fooTfmer = TfidfTransformer()\n",
    "\n",
    "# Again, fit and transform\n",
    "docs_tfidf = fooTfmer.fit_transform(text_counts)\n",
    "\n",
    "#splitting the data in test and training\n",
    "#from sklearn.model_selection() import train_test_split()\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs_tfidf, data['Sentiment'],test_size=0.25,random_state=5)\n",
    "\n",
    "#defining the model\n",
    "#compilimg the model -> we are going to use already used models GNB, MNB, CNB, BNB\n",
    "#fitting the model\n",
    "MNB.fit(x_train, y_train)\n",
    "accuracy_score_mnb = metrics.accuracy_score(MNB.predict(x_test), y_test)\n",
    "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
    "\n",
    "BNB.fit(x_train, y_train)\n",
    "accuracy_score_bnb = metrics.accuracy_score(BNB.predict(x_test), y_test)\n",
    "print('accuracy_score_bnb = '+str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
    "\n",
    "#CNB.fit(x_train, y_train)\n",
    "#accuracy_score_cnb = metrics.accuracy_score(CNB.predict(x_test), y_test)\n",
    "#print('accuracy_score_cnb = '+str('{:4.2f}'.format(accuracy_score_cnb*100))+'%')\n",
    "\n",
    "#GNB.fit(x_train.todense(), y_train)\n",
    "#accuracy_score_gnb = metrics.accuracy_score(GNB.predict(x_test.todense()), y_test)\n",
    "#print('accuracy_score_gnb = '+str('{:4.2f}'.format(accuracy_score_gnb*100))+'%')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do:  \n",
    "1. Run for the whole voted dataset on a larger compute resource(suggested by TA). Depending on need can make use of the partial fit function to do batching. \n",
    "2. Try to form a pipeline of countvectorizer, tfidf-transformer, MNB and then try to tune the parameters (ngram range,tfidf-norm,MNB alpha etc.) for the whole pipeline using GridsearchCV.\n",
    "3. Try out other optimizations and tuning if exists.  \n",
    "4. Analyze the performance of each of the three models(MNB,GNB,BNB) by using metrics like classification report etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
